{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50479d91",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Machine learning is a way of creating mathematical models that help us understand and make predictions from data. While it's often seen as part of artificial intelligence, in practice‚Äîespecially in data science‚Äîit's more useful to think of it as a tool for working with data.\n",
    "\n",
    "The \"learning\" part means that these models can adjust themselves based on the data they are given. After learning from past data, they can make predictions about new data.\n",
    "\n",
    "Before diving into different machine learning methods, it's important to understand the basic types of problems machine learning can solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9930f90",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There are mainly **two types** of machine learning:\n",
    "\n",
    "1. **Supervised Learning**:\n",
    "\n",
    "   * You give the computer both the **data** and the **answers** (called labels), so it can learn the relationship between them.\n",
    "   * It's used to predict labels for new data.\n",
    "   * There are two kinds:\n",
    "\n",
    "     * **Classification**: The answers are categories (like \"spam\" or \"not spam\").\n",
    "     * **Regression**: The answers are numbers (like predicting house prices).\n",
    "\n",
    "2. **Unsupervised Learning**:\n",
    "\n",
    "   * You only give the computer the **data**, without any labels.\n",
    "   * The goal is to find patterns or groupings in the data on its own.\n",
    "   * Two common tasks:\n",
    "\n",
    "     * **Clustering**: Finding groups of similar data points.\n",
    "     * **Dimensionality Reduction**: Making the data simpler while keeping important information.\n",
    "\n",
    "There's also a middle type called **Semi-Supervised Learning**, which is used when you have only **some** labels, not all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857e663",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **What is Classification?**\n",
    "\n",
    "**Classification** is a type of machine learning task where the goal is to predict **discrete labels** (like \"spam\" or \"not spam\") for new data points based on previously labeled data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Explained:**\n",
    "\n",
    "* Imagine you have a bunch of points on a 2D plane, each with a **color label**: red or blue.\n",
    "* Each point has two features: its **x** and **y** position.\n",
    "* The goal is to build a model that can decide whether a **new point** should be labeled red or blue.\n",
    "\n",
    "---\n",
    "\n",
    "### **How it Works:**\n",
    "\n",
    "1. **Training**:\n",
    "\n",
    "   * You assume the red and blue points can be separated with a **straight line**.\n",
    "   * You adjust the position and angle of the line (called **model parameters**) so that it best splits the red and blue points.\n",
    "   * This process is called **training the model**.\n",
    "\n",
    "   ![Training](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-2.png)\n",
    "\n",
    "2. **Prediction**:\n",
    "\n",
    "   * Once trained, you can use this model (the line) to predict the label of **new points** by checking which side of the line they fall on.\n",
    "\n",
    "   ![Prediction](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-1.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-world Example: Spam Detection**\n",
    "\n",
    "* **Features**: Word counts in an email (e.g., \"Viagra\", \"prince\").\n",
    "* **Label**: \"Spam\" or \"Not Spam\".\n",
    "* The model learns from a small labeled sample and then predicts labels for the rest.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Machine Learning?**\n",
    "\n",
    "While a human can draw a line in simple cases, machine learning can:\n",
    "\n",
    "* Handle **large** datasets.\n",
    "* Work in **many dimensions** (not just 2D).\n",
    "* Automatically find the best way to separate classes without manual effort.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53eccf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß† What is Regression?\n",
    "\n",
    "> **Regression** is a machine learning method used to **predict numbers** (like prices, temperatures, or distances).\n",
    "\n",
    "In contrast to **classification** (which predicts categories like red/blue, spam/not spam), **regression** predicts **continuous values** like `3.14`, `72.5`, `150 meters`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Simple Example:\n",
    "\n",
    "Imagine you‚Äôre a scientist and you have a dataset of points where:\n",
    "\n",
    "* Each point has **two features**: let's call them `Feature 1` and `Feature 2`.\n",
    "* Each point also has a **value** (label), like a score or measurement.\n",
    "\n",
    "We want to **predict the value** for new points using the existing data.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Step 1: The Raw Data\n",
    "\n",
    "![fig1](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-1.png)\n",
    "\n",
    "* The **x and y positions** of the dots are the two features.\n",
    "* The **color** of each point shows its value (label).\n",
    "* But right now, we don‚Äôt have any formula to predict the value for new points.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Step 2: Add the Value as a Third Dimension\n",
    "\n",
    "![fig2](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-2.png)\n",
    "\n",
    "Now we imagine the value (label) as a third direction ‚Äî like going **up and down** (z-axis).\n",
    "\n",
    "So each point now has:\n",
    "\n",
    "* Feature 1 (x)\n",
    "* Feature 2 (y)\n",
    "* Label (z) ‚Üí shown by height and color\n",
    "\n",
    "üëâ The idea: **Fit a flat surface (plane)** through these 3D points to make predictions.\n",
    "This surface is your **regression model**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úèÔ∏è Step 3: Fit the Plane to the Data\n",
    "\n",
    "![fig3](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-3.png)\n",
    "\n",
    "* The model draws a **flat plane** through the data.\n",
    "* This plane helps estimate what the label (value) will be for any new point with known Feature 1 and Feature 2.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÆ Step 4: Make Predictions\n",
    "\n",
    "Now, whenever we get a **new data point**, we can:\n",
    "\n",
    "1. Use the model (the plane) to **find the predicted value**.\n",
    "2. The higher or lower the point is (in the z-direction), the more its predicted value changes.\n",
    "\n",
    "---\n",
    "\n",
    "## üåå Real-Life Example: Predicting Galaxy Distance\n",
    "\n",
    "* **Goal**: Find how far galaxies are from Earth.\n",
    "* **Features**: Brightness of a galaxy at different colors (red, blue, etc.).\n",
    "* **Label**: Actual distance (or redshift) of the galaxy.\n",
    "\n",
    "üí° Instead of measuring each galaxy‚Äôs distance directly (which is expensive), scientists use **regression** to **predict** it based on how bright it looks in different colors.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Key Takeaways:\n",
    "\n",
    "| Concept           | Meaning                                                                 |\n",
    "| ----------------- | ----------------------------------------------------------------------- |\n",
    "| Regression        | Predicting **numbers** (continuous values).                             |\n",
    "| Features          | Input information (e.g., brightness, size, etc.).                       |\n",
    "| Labels            | The value we want to predict (e.g., price, distance).                   |\n",
    "| Linear Regression | A method that fits a **flat surface** (like a plane) to predict values. |\n",
    "| Advantage         | Works well even with **huge datasets** and **many features**.           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb7d37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üîç What is Clustering?\n",
    "\n",
    "> **Clustering** is a type of **unsupervised learning**, which means you‚Äôre working with data that has **no labels**.\n",
    "\n",
    "Instead of telling the computer what each point represents (like spam or not spam), you let the computer **find patterns or groups** on its own.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Think of it like this:\n",
    "\n",
    "Imagine you have a **bunch of dots on a page**, but you don‚Äôt know which ones belong to which group.\n",
    "\n",
    "Your goal is to **group similar dots together**, even though no one has told you what the groups are.\n",
    "\n",
    "---\n",
    "\n",
    "### üñºÔ∏è Step 1: The Raw Data (No Labels)\n",
    "\n",
    "![fig1](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-clustering-1.png)\n",
    "\n",
    "In the image above:\n",
    "\n",
    "* Each dot is just a point in space with no label.\n",
    "* But if you look at it, your **eyes can see** there are about **3 groups** (clusters).\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ Step 2: Let the Algorithm Group Them\n",
    "\n",
    "Now we use a **clustering algorithm** (like **K-Means**) to find these groups **automatically**.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° What is K-Means?\n",
    "\n",
    "* ‚ÄúK‚Äù is the **number of clusters** you want to find (e.g., K=3).\n",
    "* The algorithm:\n",
    "\n",
    "  1. Randomly places 3 centers (dots) on the graph.\n",
    "  2. Assigns every point to the **closest center**.\n",
    "  3. Moves each center to the **middle of its group**.\n",
    "  4. Repeats steps 2‚Äì3 until everything settles.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Step 3: Final Clusters\n",
    "\n",
    "![fig2](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-clustering-2.png)\n",
    "\n",
    "* The algorithm successfully finds 3 groups.\n",
    "* Each point is now colored according to the group it belongs to.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Why Clustering is Useful\n",
    "\n",
    "Even though it‚Äôs simple in 2D, this method works on **big, complex datasets** with **many dimensions**. It's used in:\n",
    "\n",
    "* üîç **Customer segmentation**: Group customers based on shopping habits.\n",
    "* üß¨ **Gene expression**: Group genes that behave similarly.\n",
    "* üì∏ **Image compression**: Reduce image size by grouping similar pixels.\n",
    "* üì∞ **Topic modeling**: Find topics in a collection of articles.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Concept      | Explanation                                                    |\n",
    "| ------------ | -------------------------------------------------------------- |\n",
    "| Clustering   | Grouping similar data points without any labels                |\n",
    "| Unsupervised | No predefined labels ‚Äî let the data ‚Äúspeak for itself‚Äù         |\n",
    "| K-Means      | A popular clustering method that groups data into `k` clusters |\n",
    "| Output       | Points grouped into clusters based on similarity               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7360ed6c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üîç What Is Dimensionality?\n",
    "\n",
    "Let‚Äôs start from the basics.\n",
    "\n",
    "* A **dimension** is just a **feature** or **column** in your data.\n",
    "* For example:\n",
    "\n",
    "  * A table with `height`, `weight`, and `age` has **3 dimensions**.\n",
    "  * An image of size 28√ó28 = 784 pixels has **784 dimensions**.\n",
    "  * A sound clip measured every 1 millisecond might have **thousands of dimensions**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Problem: High-Dimensional Data is Hard to Understand\n",
    "\n",
    "Humans can only **see and imagine in 2D or 3D**. But real-world data often has **hundreds or thousands of dimensions**.\n",
    "\n",
    "So, how do we understand it?\n",
    "\n",
    "That‚Äôs where **Dimensionality Reduction** comes in. It‚Äôs like:\n",
    "\n",
    "ü™Ñ \"Let me take your complicated data and show you a simpler version that still makes sense.\"\n",
    "\n",
    "---\n",
    "\n",
    "## üå™Ô∏è The Spiral Example\n",
    "\n",
    "Let‚Äôs understand using this spiral image:\n",
    "\n",
    "![Spiral Data](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-dimesionality-1.png)\n",
    "\n",
    "* This looks like messy 2D data.\n",
    "* But actually, these dots **follow one curved path**, like a thread rolled into a spiral.\n",
    "* So, even though it's shown in 2D, the real data has just **one variable** (you can go forward or backward along the spiral).\n",
    "\n",
    "This is called a **\"1D structure in 2D space.\"**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÇÔ∏è What Dimensionality Reduction Does\n",
    "\n",
    "A dimensionality reduction algorithm, like **Isomap**, tries to:\n",
    "\n",
    "> Flatten out the spiral, so that the same points are shown in a straight line.\n",
    "\n",
    "Here's what it looks like after applying Isomap:\n",
    "\n",
    "![Flattened Spiral](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-dimesionality-2.png)\n",
    "\n",
    "### What Do You See?\n",
    "\n",
    "* Points are now spread out on a line.\n",
    "* The **color gradient** shows that the algorithm kept the original order (start to end of the spiral).\n",
    "* It captured the **true shape** of the data, but in **just 1 dimension**!\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why This Is Powerful\n",
    "\n",
    "Imagine you had:\n",
    "\n",
    "* 1,000 features per data point (like a genome or large image).\n",
    "* You want to see if there‚Äôs a **hidden pattern**.\n",
    "* Plotting in 1,000 dimensions is impossible.\n",
    "\n",
    "So you:\n",
    "\n",
    "1. Apply dimensionality reduction.\n",
    "2. Convert the data into 2 or 3 dimensions.\n",
    "3. Plot it ‚Äî now you can **see clusters, trends, or anomalies**.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Common Algorithms for Dimensionality Reduction\n",
    "\n",
    "* **PCA (Principal Component Analysis)** ‚Äì Simplest method, finds straight-line patterns.\n",
    "* **t-SNE** ‚Äì Good for visualizing clusters.\n",
    "* **Isomap** ‚Äì Good for unfolding curved or twisted data (like the spiral).\n",
    "* **UMAP** ‚Äì Modern and very powerful.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Summary\n",
    "\n",
    "| Term                         | Explanation                                                                                        |\n",
    "| ---------------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| **Dimensionality**           | Number of features (like columns in a dataset).                                                    |\n",
    "| **High-Dimensional Data**    | Data with lots of features (e.g., 100+).                                                           |\n",
    "| **Dimensionality Reduction** | Technique to convert high-dimensional data into a smaller number (2D/3D), keeping useful patterns. |\n",
    "| **Why?**                     | To visualize or simplify complex data.                                                             |\n",
    "| **Spiral Example**           | The data looks 2D, but it‚Äôs actually just 1D curled in a spiral. Algorithm \"unrolls\" it.           |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
